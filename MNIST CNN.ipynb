{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST('', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))\n",
    "testset = datasets.MNIST('', train=False, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))\n",
    "\n",
    "\n",
    "trainloader  = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, pin_memory=True)\n",
    "testloader  = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.conv1 = nn.Conv2d(1, 64, 5)  # input channel=1, output channel=64, kernal size = 3*3\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.conv2 = nn.Conv2d(64, 32, 5)\n",
    "        self.fc1 = nn.Linear(32 * 4 * 4, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 64)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # state size. 28 * 28(input image size = 28 * 28)\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        # state size. 12 * 12\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        # state size. 4 * 4\n",
    "        x = x.view(-1, 32 * 4 * 4)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "  \n",
    "        return F.log_softmax(x, dim=1)\n",
    "      \n",
    "net = Net() # inital network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # create a Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,   100] loss: 0.051\n",
      "[1,   200] loss: 0.047\n",
      "[1,   300] loss: 0.038\n",
      "[1,   400] loss: 0.047\n",
      "[1,   500] loss: 0.047\n",
      "[1,   600] loss: 0.045\n",
      "[2,   100] loss: 0.040\n",
      "[2,   200] loss: 0.032\n",
      "[2,   300] loss: 0.039\n",
      "[2,   400] loss: 0.043\n",
      "[2,   500] loss: 0.034\n",
      "[2,   600] loss: 0.037\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(trainloader):\n",
    "        X, y = data\n",
    "\n",
    "        # training process\n",
    "        optimizer.zero_grad()    # clear the gradient calculated previously\n",
    "        predicted = net(X)  # put the mini-batch training data to Nerual Network, and get the predicted labels\n",
    "        loss = F.nll_loss(predicted, y)  # compare the predicted labels with ground-truth labels\n",
    "        loss.backward()      # compute the gradient\n",
    "        optimizer.step()     # optimize the network\n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 99:    # print every 1000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 100))\n",
    "            running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Accuracy: 59372/60000 = 0.99\n",
      "testing data Accuracy: 9886/10000 = 0.989\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model.train()\" and \"model.eval()\" activates and deactivates Dropout and BatchNorm, so it is quite important. \n",
    "\"with torch.no_grad()\" only deactivates gradient calculations, but doesn't turn off Dropout and BatchNorm.\n",
    "Your model accuracy will therefore be lower if you don't use model.eval() when evaluating the model.\n",
    "\"\"\"\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        X, y = data\n",
    "        output = net(X)\n",
    "        correct += (torch.argmax(output, dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f'Training data Accuracy: {correct}/{total} = {round(correct/total, 3)}')\n",
    "\n",
    "# Evaluation the testing data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        X, y = data\n",
    "        output = net(X)\n",
    "        correct += (torch.argmax(output, dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f'testing data Accuracy: {correct}/{total} = {round(correct/total, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
