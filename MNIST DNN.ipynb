{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms, datasets\n",
    "\n",
    "trainset = datasets.MNIST('', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))\n",
    "testset = datasets.MNIST('', train=False, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))\n",
    "\n",
    "\n",
    "trainloader  = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, pin_memory=True)\n",
    "testloader  = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.MNIST('', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))\n",
    "testset = datasets.MNIST('', train=False, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))\n",
    "\n",
    "\n",
    "trainloader  = torch.utils.data.DataLoader(trainset, batch_size=100, shuffle=True, pin_memory=True)\n",
    "testloader  = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=True, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(28 * 28, 64)  # the image size is 28 by 28\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)\n",
    "      \n",
    "net = Net() # inital network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # create a Adam optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0, loss:0.12780500948429108\n",
      "epoch:1, loss:0.2258947491645813\n"
     ]
    }
   ],
   "source": [
    "net.train()\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    for data in trainloader:\n",
    "        X, y = data\n",
    "        # training process\n",
    "        optimizer.zero_grad()    # clear the gradient calculated previously\n",
    "        predicted = net(X.view(-1, 28 * 28))  # put the mini-batch training data to Nerual Network, and get the predicted labels\n",
    "        loss = F.nll_loss(predicted, y)  # compare the predicted labels with ground-truth labels\n",
    "        loss.backward()      # compute the gradient\n",
    "        optimizer.step()     # optimize the network\n",
    "    print(f'epoch:{epoch}, loss:{loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Accuracy: 57143/60000 = 0.952\n",
      "testing data Accuracy: 9473/10000 = 0.947\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "model.train()\" and \"model.eval()\" activates and deactivates Dropout and BatchNorm, so it is quite important. \n",
    "\"with torch.no_grad()\" only deactivates gradient calculations, but doesn't turn off Dropout and BatchNorm.\n",
    "Your model accuracy will therefore be lower if you don't use model.eval() when evaluating the model.\n",
    "\"\"\"\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in trainloader:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28 * 28))\n",
    "        correct += (torch.argmax(output, dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f'Training data Accuracy: {correct}/{total} = {round(correct/total, 3)}')\n",
    "\n",
    "# Evaluation the testing data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testloader:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28 * 28))\n",
    "        correct += (torch.argmax(output, dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f'testing data Accuracy: {correct}/{total} = {round(correct/total, 3)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
