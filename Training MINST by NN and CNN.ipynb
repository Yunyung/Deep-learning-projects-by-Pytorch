{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x27e9240e4e0>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms, datasets\n",
    "import time\n",
    "train = datasets.MNIST('', train=True, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))\n",
    "test = datasets.MNIST('', train=False, download=True, \n",
    "                       transform=transforms.Compose([\n",
    "                                transforms.ToTensor()\n",
    "                            ]))\n",
    "\n",
    "trainset = torch.utils.data.DataLoader(train, batch_size=100, shuffle=True, pin_memory=True)\n",
    "testset = torch.utils.data.DataLoader(test, batch_size=100, shuffle=True, pin_memory=True)\n",
    "                                      \n",
    "# show an image for example\n",
    "import matplotlib.pyplot as plt\n",
    "X, y = iter(trainset).next()\n",
    "print(X.shape) # the batch is 10 example. And each example is 28 * 28 pixels\n",
    "plt.imshow(X[0].view(28, 28)) # the image size is 28 by 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # an affine operation: y = Wx + b\n",
    "        self.fc1 = nn.Linear(28 * 28, 64)  # the image size is 28 by 28\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return F.log_softmax(x, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.1997, grad_fn=<NllLossBackward>)\n",
      "tensor(0.2047, grad_fn=<NllLossBackward>)\n",
      "CPU Spent Time: 19.554386\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "net = Net() # inital network \n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # create a Adam optimizer\n",
    "\n",
    "net.train()\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        # training process\n",
    "        optimizer.zero_grad()    # clear the gradient calculated previously\n",
    "        predicted = net(X.view(-1, 28 * 28))  # put the mini-batch training data to Nerual Network, and get the predicted labels\n",
    "        loss = F.nll_loss(predicted, y)  # compare the predicted labels with ground-truth labels\n",
    "        loss.backward()      # compute the gradient\n",
    "        optimizer.step()     # optimize the network\n",
    "    print(loss)\n",
    "    \n",
    "print(\"CPU Spent Time: %f\" % (time.time() - start_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Accuracy: 57105/60000 = 0.952\n",
      "testing data Accuracy: 9503/10000 = 0.95\n",
      "CPU Spent Time: 8.806992\n"
     ]
    }
   ],
   "source": [
    "# Evaluation the training data\n",
    "start_time = time.time()\n",
    "\"\"\"\n",
    "model.train()\" and \"model.eval()\" activates and deactivates Dropout and BatchNorm, so it is quite important. \n",
    "\"with torch.no_grad()\" only deactivates gradient calculations, but doesn't turn off Dropout and BatchNorm.\n",
    "Your model accuracy will therefore be lower if you don't use model.eval() when evaluating the model.\n",
    "\"\"\"\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28 * 28))\n",
    "        correct += (torch.argmax(output, dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f'Training data Accuracy: {correct}/{total} = {round(correct/total, 3)}')\n",
    "\n",
    "# Evaluation the testing data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        output = net(X.view(-1, 28 * 28))\n",
    "        correct += (torch.argmax(output, dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f'testing data Accuracy: {correct}/{total} = {round(correct/total, 3)}')\n",
    "\n",
    "print(\"CPU Spent Time: %f\" % (time.time() - start_time))     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n",
      "tensor(0.2444, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "tensor(0.0567, device='cuda:0', grad_fn=<NllLossBackward>)\n",
      "GPU Spent Time: 19.941451\n"
     ]
    }
   ],
   "source": [
    "# The GPU version\n",
    "torch.cuda.init()\n",
    "start_time = time.time()\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "net = Net() # inital network \n",
    "net.to(device) #these methods will recursively go over all modules and convert their parameters and buffers to CUDA tensors:\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)  # create a Adam optimizer\n",
    "\n",
    "net.train()\n",
    "epochs = 2\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        X, y = X.to(device), y.to(device)   # Remember that you will have to send the inputs and targets at every step to the GPU too\n",
    "        # training process\n",
    "        optimizer.zero_grad()    # clear the gradient calculated previously\n",
    "        predicted = net(X.view(-1, 28 * 28))  # put the mini-batch training data to Nerual Network, and get the predicted labels\n",
    "        loss = F.nll_loss(predicted, y)  # compare the predicted labels with ground-truth labels\n",
    "        loss.backward()      # compute the gradient\n",
    "        optimizer.step()     # optimize the network\n",
    "    print(loss)\n",
    "    \n",
    "print(\"GPU Spent Time: %f\" % (time.time() - start_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data Accuracy: 57221/60000 = 0.954\n",
      "testing data Accuracy: 9486/10000 = 0.949\n",
      "GPU Spent Time: 8.062385\n"
     ]
    }
   ],
   "source": [
    "# Evaluation the training data\n",
    "start_time = time.time()\n",
    "\n",
    "net.eval()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in trainset:\n",
    "        X, y = data\n",
    "        X, y = X.to(device), y.to(device)   # Remember that you will have to send the inputs and targets at every step to the GPU too\n",
    "        output = net(X.view(-1, 28 * 28))\n",
    "        correct += (torch.argmax(output, dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f'Training data Accuracy: {correct}/{total} = {round(correct/total, 3)}')\n",
    "\n",
    "# Evaluation the testing data\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for data in testset:\n",
    "        X, y = data\n",
    "        X, y = X.to(device), y.to(device)   # Remember that you will have to send the inputs and targets at every step to the GPU too\n",
    "        output = net(X.view(-1, 28 * 28))\n",
    "        correct += (torch.argmax(output, dim=1) == y).sum().item()\n",
    "        total += y.size(0)\n",
    "\n",
    "print(f'testing data Accuracy: {correct}/{total} = {round(correct/total, 3)}')\n",
    "\n",
    "print(\"GPU Spent Time: %f\" % (time.time() - start_time))    \n",
    "\n",
    "# Note:\n",
    "# In this projects the GPU is not much faster than CPU, the one reason is that this nerual network is very small, \n",
    "# and the batch size also effect the speed. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
